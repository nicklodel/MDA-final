{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-11T16:21:19.862052879Z",
     "start_time": "2023-05-11T16:21:19.106534979Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from skmultilearn.dataset import load_dataset,available_data_sets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "bibtex:train - exists, not redownloading\n",
      "bibtex:test - exists, not redownloading\n",
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "delicious:train - exists, not redownloading\n",
      "delicious:test - exists, not redownloading\n",
      "enron:train - exists, not redownloading\n",
      "enron:test - exists, not redownloading\n",
      "genbase:train - exists, not redownloading\n",
      "genbase:test - exists, not redownloading\n",
      "rcv1subset1:train - exists, not redownloading\n",
      "rcv1subset1:test - exists, not redownloading\n",
      "medical:train - exists, not redownloading\n",
      "medical:test - exists, not redownloading\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "tmc2007_500:train - exists, not redownloading\n",
      "tmc2007_500:test - exists, not redownloading\n",
      "Corel5k:train - exists, not redownloading\n",
      "Corel5k:test - exists, not redownloading\n",
      "tmc2007_500:train - exists, not redownloading\n",
      "tmc2007_500:test - exists, not redownloading\n",
      "yeast:train - exists, not redownloading\n",
      "yeast:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_trainEmotions, y_trainEmotions, feature_namesEmotions, label_namesEmotions = load_dataset('emotions', 'train')\n",
    "X_testEmotions, y_testEmotions, _, _ = load_dataset('emotions', 'test')\n",
    "\n",
    "X_trainBibtex, y_trainBibtex, feature_namesBibtex, label_namesBibtex = load_dataset('bibtex', 'train')\n",
    "X_testBibtex, y_testBibtex, _, _ = load_dataset('bibtex', 'test')\n",
    "\n",
    "X_trainBirds, y_trainBirds, feature_namesBirds, label_namesBirds = load_dataset('birds', 'train')\n",
    "X_testBirds, y_testBirds, _, _ = load_dataset('birds', 'test')\n",
    "\n",
    "X_trainDelicious, y_trainDelicious, feature_namesDelicious, label_namesDelicious = load_dataset('delicious', 'train')\n",
    "X_testDelicious, y_testDelicious, _, _ = load_dataset('delicious', 'test')\n",
    "\n",
    "X_trainEnron, y_trainEnron, feature_namesEnron, label_namesEnron = load_dataset('enron', 'train')\n",
    "X_testEnron, y_testEnron, _, _ = load_dataset('enron', 'test')\n",
    "\n",
    "X_trainGenbase, y_trainGenbase, feature_namesGenbase, label_namesGenbase = load_dataset('genbase', 'train')\n",
    "X_testGenbase, y_testGenbase, _, _ = load_dataset('genbase', 'test')\n",
    "\n",
    "X_trainRcv1, y_trainRcv1, feature_namesRcv1, label_namesRcv1 = load_dataset('rcv1subset1', 'train')\n",
    "X_testRcv1, y_testRcv1, _, _ = load_dataset('rcv1subset1', 'test')\n",
    "\n",
    "X_trainMedical, y_trainMedical, feature_namesMedical, label_namesMedical = load_dataset('medical', 'train')\n",
    "X_testMedical, y_testMedical, _, _ = load_dataset('medical', 'test')\n",
    "\n",
    "X_trainScene, y_trainScene, feature_namesScene, label_namesScene = load_dataset('scene', 'train')\n",
    "X_testScene, y_testScene, _, _ = load_dataset('scene', 'test')\n",
    "\n",
    "X_trainTmc, y_trainTmc, feature_namesTmc, label_namesTmc = load_dataset('tmc2007_500', 'train')\n",
    "X_testTmc, y_testTmc, _, _ = load_dataset('tmc2007_500', 'test')\n",
    "\n",
    "X_trainCorel5k, y_trainCorel5k, feature_namesCorel5k, label_namesCorel5k = load_dataset('Corel5k', 'train')\n",
    "X_testCorel5k, y_testCorel5k, _, _ = load_dataset('Corel5k', 'test')\n",
    "\n",
    "X_trainTmc, y_trainTmc, feature_namesTmc, label_namesTmc = load_dataset('tmc2007_500', 'train')\n",
    "X_testTmc, y_testTmc, _, _ = load_dataset('tmc2007_500', 'test')\n",
    "\n",
    "X_trainYeast, y_trainYeast, feature_namesYeast, label_namesYeast = load_dataset('yeast','train')\n",
    "X_testYeast, y_testYeast,_,_ = load_dataset('yeast','test')\n",
    "data = [\n",
    "    ('Corel5k',X_trainCorel5k,y_trainCorel5k,X_testCorel5k,y_testCorel5k),\n",
    "    ('genbase',X_trainGenbase,y_trainGenbase,X_testGenbase,y_testGenbase),\n",
    "    ('rcv1subset1',X_trainRcv1,y_trainRcv1,X_testRcv1,y_testRcv1),\n",
    "    ('Medical',X_trainMedical,y_trainMedical,X_testMedical,y_testMedical),\n",
    "    ('enron',X_trainEnron,y_trainEnron,X_testEnron,y_testEnron),\n",
    "    ('Emotions',X_trainEmotions,y_trainEmotions,X_testEmotions,y_testEmotions),\n",
    "        ('Bibtex',X_trainBibtex,y_trainBibtex,X_testBibtex,y_testBibtex),\n",
    "        ('birds',X_trainBirds,y_trainBirds,X_testBirds,y_testBirds),\n",
    "        ('Yeast', X_trainYeast,y_trainYeast,X_testYeast,y_testYeast),\n",
    "        ('scene',X_trainScene,y_trainScene,X_testScene,y_testScene),\n",
    "]\n",
    "domains = ['Corel5k','genbase','Rcv1subset1','Medical','enron','Emotions','bibtex','birds','Yeast','scene']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T16:21:33.453584211Z",
     "start_time": "2023-05-11T16:21:19.849294832Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "mlp =  MLPClassifier(hidden_layer_sizes=(10,10,))\n",
    "brknn = BinaryRelevance(classifier=knn, require_dense=[False, True])\n",
    "brdt = BinaryRelevance(classifier=dt,require_dense=[False,True])\n",
    "brmlp = BinaryRelevance(classifier=mlp,require_dense=[False,True])\n",
    "lp = LabelPowerset()\n",
    "mlknn = MLkNN(k=3,s=1.0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T16:21:33.642610899Z",
     "start_time": "2023-05-11T16:21:33.301971860Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Elija tres métricas de las estudiadas en teoría para evaluar el rendimiento de los métodos. Una al menos de esasmedidas ha de ser implementada por el alumno."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binary Relevance con knn y evaluamos coverage,accuracy y hamming loss de manera manual y con scikit learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # 2. En primer lugar vamos a comparar el efecto en el rendimiento de los métodos de transformación de la elección del clasificador base. Ejecute el método BR con tres clasificadores base de los disponibles en scikit-learn y compare el rendimiento en los problemas seleccionados usando las tres métricas del apartado anterior."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR with Knn of the dataset:  Corel5k\n",
      "Hamming loss manual:  0.010620320855614974\n",
      "Hamming loss:  0.010620320855614974\n",
      "Accuracy:  0.008\n",
      "Coverage:  368.806\n",
      "BR with Knn of the dataset:  genbase\n",
      "Hamming loss manual:  0.04652894100130281\n",
      "Hamming loss:  0.04652894100130281\n",
      "Accuracy:  0.2814070351758794\n",
      "Coverage:  19.683417085427134\n",
      "BR with Knn of the dataset:  rcv1subset1\n",
      "Hamming loss manual:  0.043148514851485145\n",
      "Hamming loss:  0.043148514851485145\n",
      "Accuracy:  0.0003333333333333333\n",
      "Coverage:  100.934\n",
      "BR with Knn of the dataset:  Medical\n",
      "Hamming loss manual:  0.018949181739879414\n",
      "Hamming loss:  0.018949181739879414\n",
      "Accuracy:  0.4170542635658915\n",
      "Coverage:  25.345736434108527\n",
      "BR with Knn of the dataset:  enron\n",
      "Hamming loss manual:  0.05823312803467266\n",
      "Hamming loss:  0.05823312803467266\n",
      "Accuracy:  0.1381692573402418\n",
      "Coverage:  45.06563039723662\n",
      "BR with Knn of the dataset:  Emotions\n",
      "Hamming loss manual:  0.2953795379537954\n",
      "Hamming loss:  0.2953795379537954\n",
      "Accuracy:  0.19306930693069307\n",
      "Coverage:  4.826732673267327\n",
      "BR with Knn of the dataset:  Bibtex\n",
      "Hamming loss manual:  0.015717018642859824\n",
      "Hamming loss:  0.015717018642859824\n",
      "Accuracy:  0.052485089463220676\n",
      "Coverage:  149.17296222664015\n",
      "BR with Knn of the dataset:  birds\n",
      "Hamming loss manual:  0.06387485742219326\n",
      "Hamming loss:  0.06387485742219326\n",
      "Accuracy:  0.44891640866873067\n",
      "Coverage:  9.746130030959753\n",
      "BR with Knn of the dataset:  Yeast\n",
      "Hamming loss manual:  0.2197382769901854\n",
      "Hamming loss:  0.2197382769901854\n",
      "Accuracy:  0.20610687022900764\n",
      "Coverage:  10.897491821155944\n",
      "BR with Knn of the dataset:  scene\n",
      "Hamming loss manual:  0.10869565217391304\n",
      "Hamming loss:  0.10869565217391304\n",
      "Accuracy:  0.6003344481605352\n",
      "Coverage:  2.948160535117057\n"
     ]
    }
   ],
   "source": [
    "brknnAccuracy = []\n",
    "brknnHammingLoss = []\n",
    "brknnCoverage = []\n",
    "for i in data:\n",
    "#  Binary relevance method with Decision tree\n",
    "\n",
    "    brknn.fit(i[1], i[2])\n",
    "\n",
    "    prediction = brknn.predict(i[3])\n",
    "\n",
    "    true_labels = np.array(i[4].todense())\n",
    "    predicted_labels = np.array(prediction.todense())\n",
    "    hamming_loss_manual = np.mean(np.not_equal(true_labels, predicted_labels))\n",
    "\n",
    "    print(\"BR with Knn of the dataset: \",i[0])\n",
    "    print(\"Hamming loss manual: \", hamming_loss_manual)\n",
    "    brknnHammingLoss.append(hamming_loss_manual)\n",
    "    print(\"Hamming loss: \", metrics.hamming_loss(i[4],prediction))\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(i[4], prediction))\n",
    "    brknnAccuracy.append(metrics.accuracy_score(i[4], prediction))\n",
    "    print(\"Coverage: \", metrics.coverage_error(i[4].toarray(), prediction.toarray()))\n",
    "    brknnCoverage.append(metrics.coverage_error(i[4].toarray(), prediction.toarray()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T16:25:12.296990468Z",
     "start_time": "2023-05-11T16:21:33.454347247Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mismas comprobaciones pero ahora el BR es con Decision tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR with Decision Tree of the dataset:  Corel5k\n",
      "Hamming loss manual:  0.01688235294117647\n",
      "Hamming loss:  0.01688235294117647\n",
      "Accuracy:  0.0\n",
      "Coverage:  368.852\n",
      "BR with Decision Tree of the dataset:  genbase\n",
      "Hamming loss manual:  0.0013028103480364787\n",
      "Hamming loss:  0.0013028103480364787\n",
      "Accuracy:  0.964824120603015\n",
      "Coverage:  1.7336683417085428\n",
      "BR with Decision Tree of the dataset:  rcv1subset1\n",
      "Hamming loss manual:  0.048498349834983495\n",
      "Hamming loss:  0.048498349834983495\n",
      "Accuracy:  0.0\n",
      "Coverage:  100.48033333333333\n",
      "BR with Decision Tree of the dataset:  Medical\n",
      "Hamming loss manual:  0.012265288544358311\n",
      "Hamming loss:  0.012265288544358311\n",
      "Accuracy:  0.6077519379844961\n",
      "Coverage:  12.612403100775193\n",
      "BR with Decision Tree of the dataset:  enron\n",
      "Hamming loss manual:  0.07068139603089256\n",
      "Hamming loss:  0.07068139603089256\n",
      "Accuracy:  0.08808290155440414\n",
      "Coverage:  44.21588946459413\n",
      "BR with Decision Tree of the dataset:  Emotions\n",
      "Hamming loss manual:  0.29455445544554454\n",
      "Hamming loss:  0.29455445544554454\n",
      "Accuracy:  0.12871287128712872\n",
      "Coverage:  4.965346534653466\n",
      "BR with Decision Tree of the dataset:  Bibtex\n",
      "Hamming loss manual:  0.01993573152281281\n",
      "Hamming loss:  0.01993573152281281\n",
      "Accuracy:  0.10377733598409543\n",
      "Coverage:  121.92842942345925\n",
      "BR with Decision Tree of the dataset:  birds\n",
      "Hamming loss manual:  0.06371191135734072\n",
      "Hamming loss:  0.06371191135734072\n",
      "Accuracy:  0.38699690402476783\n",
      "Coverage:  7.894736842105263\n",
      "BR with Decision Tree of the dataset:  Yeast\n",
      "Hamming loss manual:  0.28104066053902477\n",
      "Hamming loss:  0.28104066053902477\n",
      "Accuracy:  0.05016357688113413\n",
      "Coverage:  12.606324972737186\n",
      "BR with Decision Tree of the dataset:  scene\n",
      "Hamming loss manual:  0.15705128205128205\n",
      "Hamming loss:  0.15705128205128205\n",
      "Accuracy:  0.358695652173913\n",
      "Coverage:  3.3946488294314383\n"
     ]
    }
   ],
   "source": [
    "brdtAccuracy = []\n",
    "brdtHammingLoss = []\n",
    "brdtCoverage = []\n",
    "for i in data:\n",
    "#  Binary relevance method with Decision tree\n",
    "    brdt.fit(i[1], i[2])\n",
    "\n",
    "    prediction = brdt.predict(i[3])\n",
    "\n",
    "    true_labels = np.array(i[4].todense())\n",
    "    predicted_labels = np.array(prediction.todense())\n",
    "    hamming_loss_manual = np.mean(np.not_equal(true_labels, predicted_labels))\n",
    "\n",
    "    print(\"BR with Decision Tree of the dataset: \",i[0])\n",
    "    print(\"Hamming loss manual: \", hamming_loss_manual)\n",
    "    brdtHammingLoss.append(hamming_loss_manual)\n",
    "    print(\"Hamming loss: \", metrics.hamming_loss(i[4],prediction))\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(i[4], prediction))\n",
    "    brdtAccuracy.append(metrics.accuracy_score(i[4], prediction))\n",
    "    print(\"Coverage: \", metrics.coverage_error(i[4].toarray(), prediction.toarray()))\n",
    "    brdtCoverage.append(metrics.coverage_error(i[4].toarray(), prediction.toarray()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T16:26:02.738758895Z",
     "start_time": "2023-05-11T16:25:12.300531548Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente BR con MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR with MLP of the dataset:  Corel5k\n",
      "Hamming loss manual:  0.011368983957219251\n",
      "Hamming loss:  0.011368983957219251\n",
      "Accuracy:  0.008\n",
      "Coverage:  370.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/nicklodel/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR with MLP of the dataset:  genbase\n",
      "Hamming loss manual:  0.03108133258887028\n",
      "Hamming loss:  0.03108133258887028\n",
      "Accuracy:  0.3065326633165829\n",
      "Coverage:  18.532663316582916\n"
     ]
    }
   ],
   "source": [
    "brmlpAccuracy = []\n",
    "brmlpHammingLoss = []\n",
    "brmlpCoverage = []\n",
    "for i in data:\n",
    "#  Binary relevance method with Random Forest\n",
    "    brmlp.fit(i[1], i[2])\n",
    "\n",
    "    prediction = brmlp.predict(i[3])\n",
    "\n",
    "    true_labels = np.array(i[4].todense())\n",
    "    predicted_labels = np.array(prediction.todense())\n",
    "    hamming_loss_manual = np.mean(np.not_equal(true_labels, predicted_labels))\n",
    "\n",
    "    print(\"BR with MLP of the dataset: \",i[0])\n",
    "    print(\"Hamming loss manual: \", hamming_loss_manual)\n",
    "    brmlpHammingLoss.append(hamming_loss_manual)\n",
    "    print(\"Hamming loss: \", metrics.hamming_loss(i[4],prediction))\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(i[4], prediction))\n",
    "    brmlpAccuracy.append(metrics.accuracy_score(i[4], prediction))\n",
    "    print(\"Coverage: \", metrics.coverage_error(i[4].toarray(), prediction.toarray()))\n",
    "    brmlpCoverage.append(metrics.coverage_error(i[4].toarray(), prediction.toarray()))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-11T16:26:02.745321623Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a line chart with two series of data\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(brdtHammingLoss, label='BR with Decision Tree')\n",
    "plt.plot(brmlpHammingLoss, label='BR with MLP')\n",
    "plt.plot(brknnHammingLoss, label='BR with Knn')\n",
    "\n",
    "# Add a title and axis labels\n",
    "plt.title('Hamming Loss')\n",
    "plt.xlabel('Dominio')\n",
    "plt.ylabel('Hamming loss')\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(range(len(domains)), domains)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a line chart with two series of data\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(brdtAccuracy, label='BR with Decision Tree')\n",
    "plt.plot(brmlpAccuracy, label='BR with MLP')\n",
    "plt.plot(brknnAccuracy, label='BR with Knn')\n",
    "\n",
    "# Add a title and axis labels\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Dominio')\n",
    "plt.ylabel('Accuracy %')\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(range(len(domains)), domains)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a line chart with two series of data\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(brdtCoverage, label='BR with Decision Tree')\n",
    "plt.plot(brmlpCoverage, label='BR with MLP')\n",
    "plt.plot(brknnCoverage, label='BR with Knn')\n",
    "\n",
    "# Add a title and axis labels\n",
    "plt.title('Coverage')\n",
    "plt.xlabel('Dominio')\n",
    "plt.ylabel('Coverage %')\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(range(len(domains)), domains)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Seleccione la mejor combinación del ejercicio anterior y compárela con los métodos LP y MLkNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En coverage no existe una gran diferencia entre las tres versiones de Binary Relevance. En accuracy, los mejores resultados en 2 datasets y los peores resultados en otros dos son obtenidos por Decision tree. Por último, el que menor hamming loss tiene en average es la versión con Knn. Por motivos de similaridad en todos los resultados y del tiempo empleado junto con el accuracy, voy a usar la versión con Knn para el ejercicio 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lpAccuracy = []\n",
    "lpHammingLoss = []\n",
    "lpCoverage = []\n",
    "for i in data:\n",
    "\n",
    "# initialize LabelPowerset multi-label classifier with a RandomForest\n",
    "    classifier = LabelPowerset(\n",
    "    classifier = dt,\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "    # train\n",
    "    classifier.fit(i[1], i[2])\n",
    "\n",
    "# predict\n",
    "    prediction = classifier.predict(i[3])\n",
    "\n",
    "\n",
    "\n",
    "    true_labels = np.array(i[4].todense())\n",
    "    predicted_labels = np.array(prediction.todense())\n",
    "    hamming_loss_manual = np.mean(np.not_equal(true_labels, predicted_labels))\n",
    "\n",
    "    print(\"Label Powerset\",i[0])\n",
    "    print(\"Hamming loss manual: \", hamming_loss_manual)\n",
    "    lpHammingLoss.append(hamming_loss_manual)\n",
    "    print(\"Hamming loss: \", metrics.hamming_loss(i[4],prediction))\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(i[4], prediction))\n",
    "    lpAccuracy.append(metrics.accuracy_score(i[4], prediction))\n",
    "    print(\"Coverage: \", metrics.coverage_error(i[4].toarray(), prediction.toarray()))\n",
    "    lpCoverage.append(metrics.coverage_error(i[4].toarray(), prediction.toarray()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlknnAccuracy = []\n",
    "mlknnHammingLoss = []\n",
    "mlknnCoverage = []\n",
    "for i in data:\n",
    "    #  Mlknn\n",
    "\n",
    "    # Normalize the input feature\n",
    "    mlknn.fit(i[1],i[2])\n",
    "    prediction = mlknn.predict(i[3])\n",
    "\n",
    "    true_labels = np.array(i[4].todense())\n",
    "    predicted_labels = np.array(prediction.todense())\n",
    "    hamming_loss_manual = np.mean(np.not_equal(true_labels, predicted_labels))\n",
    "\n",
    "    print(\"mlknn del dominio: \",i[0])\n",
    "    print(\"Hamming loss manual: \", hamming_loss_manual)\n",
    "    mlknnHammingLoss.append(hamming_loss_manual)\n",
    "    print(\"Hamming loss: \", metrics.hamming_loss(i[4],prediction))\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(i[4], prediction))\n",
    "    mlknnAccuracy.append(metrics.accuracy_score(i[4], prediction))\n",
    "    print(\"Coverage: \", metrics.coverage_error(i[4].toarray(), prediction.toarray()))\n",
    "\n",
    "    mlknnCoverage.append(metrics.coverage_error(i[4].toarray(), prediction.toarray()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a line chart with two series of data\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(mlknnCoverage, label='MlKNN')\n",
    "plt.plot(lpCoverage, label='Label Powerset ')\n",
    "plt.plot(brknnCoverage, label='BR with Knn')\n",
    "\n",
    "# Add a title and axis labels\n",
    "plt.title('Coverage')\n",
    "plt.xlabel('Dominio')\n",
    "plt.ylabel('Coverage %')\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(range(len(domains)), domains)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a line chart with two series of data\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(mlknnAccuracy, label='MlKNN')\n",
    "plt.plot(lpAccuracy, label='Label Powerset ')\n",
    "plt.plot(brknnAccuracy, label='BR with Knn')\n",
    "# Add a title and axis labels\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Dominio')\n",
    "plt.ylabel('Accuracy %')\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(range(len(domains)), domains)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a line chart with two series of data\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(mlknnHammingLoss, label='MlKNN')\n",
    "plt.plot(lpHammingLoss, label='Label Powerset ')\n",
    "plt.plot(brknnHammingLoss, label='BR with Knn')\n",
    "# Add a title and axis labels\n",
    "plt.title('Hamming Loss')\n",
    "plt.xlabel('Dominio')\n",
    "plt.ylabel('Hamming Loss %')\n",
    "\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "plt.xticks(range(len(domains)), domains)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Compare gráficamente los resultados más significativos de los ejercicios anteriores usando cualquiera de las representaciones gráficas que conozca."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se ha ido haciendo durante el desarrollo de las prácticas.\n",
    "\n",
    "Con ayuda de las gráficas podemos observar la superioridad de Label Powerset en al menos 3/10 dominios con una diferencia significativa. En Coverage la curva de los 3 es bastante similar por lo que podemos intuir que es más dependiente del dataset que del propio método multilabel utilizado. En Hamming Loss LP consigue el mejor y el peor resultado y en el resto se mantiene igual al resto de métodos."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
